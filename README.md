A lightweight, high-performance repository for training and fine-tuning medium-sized GPT models. This project is a streamlined rewrite of minGPT, with a focus on practicality and performance over educational clarity. Although still actively being developed, the train.py script can already reproduce GPT-2 (124M) on the OpenWebText dataset using a single 8xA100 40GB GPU node in approximately four days. The codebase is intentionally minimal and easy to followâ€”train.py contains a ~300-line training loop, while model.py holds a ~300-line GPT architecture implementation, with optional support for loading pretrained GPT-2 weights from OpenAI. That's all it takes.
